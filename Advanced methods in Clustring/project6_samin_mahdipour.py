# -*- coding: utf-8 -*-
"""Project6-Samin Mahdipour

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uTHqSQkcUfYTR_iR33yGOBf6JrR6SldU

# Advanced Methods in Clustring
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from google.colab import drive
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
# %matplotlib inline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score

"""## Importing Dataset"""

drive.mount('/content/gdrive')
data = pd.read_csv('gdrive/My Drive/buntyshahauto-insurance-claims-data.csv')

"""## EDA:"""

data

data.info()

data.describe()

"""## Preprocessing"""

data.isnull().sum()

if data.duplicated().any():
    print(data.duplicated().sum(),"has duplicate data.")
else:
    print("no duplicate.")

column_delete = ['_c39', 'policy_number', 'policy_bind_date', 'policy_csl', 'incident_date', 'incident_location',"insured_zip", "insured_hobbies", 'incident_city', 'auto_make', 'auto_model', 'auto_year']
data = data.drop(columns=column_delete)

data.head()

data['fraud_reported'].value_counts()

"""## Clustering"""

X = data.drop(columns=['fraud_reported'])
y = data['fraud_reported']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
y_train_resampled.value_counts()

numerical= X.select_dtypes(include='number').columns
categorical= X.select_dtypes(include='object').columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical),
        ('cat', OneHotEncoder(drop='first', sparse=False), categorical)
    ])
X_train_encoded = preprocessor.fit_transform(X_train_resampled)
X_test_encoded = preprocessor.transform(X_test)

"""**K-Means**"""

from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X_train_encoded)

# Calculate Silhouette Score
kmeans_score = silhouette_score(X_train_encoded, kmeans_labels)

print(f"K-Means Silhouette Score: {kmeans_score}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'Y' represents the positive class
kmeans_conf_matrix = confusion_matrix(y_train_resampled.map({'Y': 1, 'N': 0}), kmeans_labels)
sns.heatmap(kmeans_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix - KMeans')
plt.show()

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Apply PCA for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_encoded)

# Visualize KMeans clusters
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=kmeans_labels, cmap='viridis', edgecolors='k')
plt.title('KMeans Clustering')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

y_true_binary = np.where(y_train_resampled == 'Y', 1, 0)
print("K-Means:")
print(classification_report(y_true_binary, kmeans_labels))

"""**Agglonerative Clustering**"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Create Agglomerative Clustering model
agg_clustering = AgglomerativeClustering(n_clusters=2)
agg_labels = agg_clustering.fit_predict(X_train_encoded)

# Calculate Silhouette Score
agg_score = silhouette_score(X_train_encoded, agg_labels)

print(f"Agglomerative Hierarchical Silhouette Score: {agg_score}")

agg_conf_matrix = confusion_matrix(y_train_resampled.map({'Y': 1, 'N': 0}), agg_labels)
sns.heatmap(agg_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix - Agglomerative Clustering')
plt.show()

# Apply PCA for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_encoded)

# Visualize Agglomerative Clustering clusters
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=agg_labels, cmap='viridis', edgecolors='k')
plt.title('Agglomerative Clustering')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

y_true_binary = np.where(y_train_resampled == 'Y', 1, 0)
print("Agglomerative Hierarchical:")
print(classification_report(y_true_binary, agg_labels))

"""**DB-Scan**"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Create DBSCAN model
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_train_encoded)

# Calculate Silhouette Score
dbscan_score = silhouette_score(X_train_encoded, dbscan_labels)

print(f"DBSCAN Silhouette Score: {dbscan_score}")

# Assuming 'Y' represents the positive class
dbscan_conf_matrix = confusion_matrix(y_train_resampled.map({'Y': 1, 'N': 0}), np.where(dbscan_labels == -1, 0, 1))
sns.heatmap(dbscan_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix - DBSCAN')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Visualize DBSCAN clusters with noise
unique_labels = np.unique(dbscan_labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

# Apply PCA for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_encoded)

plt.figure(figsize=(8, 6))

for label, color in zip(unique_labels, colors):
    if label == -1:
        plt.scatter(X_train_pca[dbscan_labels == label, 0], X_train_pca[dbscan_labels == label, 1],
                    c='gray', edgecolors='k', s=20, alpha=0.5, label='Noise')
    else:
        plt.scatter(X_train_pca[dbscan_labels == label, 0], X_train_pca[dbscan_labels == label, 1],
                    c=color, edgecolors='k', s=50, label=f'Cluster {label}')

plt.title('DBSCAN Clustering')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

y_true_binary = np.where(y_train_resampled == 'Y', 1, 0)
print(" DBSCAN:")
print(classification_report(y_true_binary, dbscan_labels))

"""## Sensitivity Analysis"""

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score

# Sensitivity Analysis for KMeans
kmeans_clusters_values = range(2, 10)
kmeans_sensitivity_results = []

for num_clusters in kmeans_clusters_values:
    kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans_labels = kmeans_model.fit_predict(X_train_encoded)
    silhouette = silhouette_score(X_train_encoded, kmeans_labels)
    kmeans_sensitivity_results.append((num_clusters, silhouette))

# Sensitivity Analysis for Agglomerative Clustering
agg_linkage_values = ['ward', 'complete', 'average', 'single']
agg_sensitivity_results = []

for linkage_type in agg_linkage_values:
    agg_model = AgglomerativeClustering(n_clusters=2, linkage=linkage_type)
    agg_labels = agg_model.fit_predict(X_train_encoded)
    silhouette = silhouette_score(X_train_encoded, agg_labels)
    agg_sensitivity_results.append((linkage_type, silhouette))

# Sensitivity Analysis for DBSCAN
dbscan_eps_values = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5]
dbscan_sensitivity_results = []

for eps_value in dbscan_eps_values:
    dbscan_model = DBSCAN(eps=eps_value, min_samples=5)
    dbscan_labels = dbscan_model.fit_predict(X_train_encoded)
    silhouette = silhouette_score(X_train_encoded, dbscan_labels)
    dbscan_sensitivity_results.append((eps_value, silhouette))

# Print the results
print("Sensitivity Analysis for K-Means:")
for result in kmeans_sensitivity_results:
    print(f"Number of Clusters: {result[0]}, Silhouette Score: {result[1]:.4f}")

print("\nSensitivity Analysis for Agglomerative Hierarchical:")
for result in agg_sensitivity_results:
    print(f"Linkage Type: {result[0]}, Silhouette Score: {result[1]:.4f}")

print("\nSensitivity Analysis for DBSCAN:")
for result in dbscan_sensitivity_results:
    print(f"Epsilon: {result[0]}, Silhouette Score: {result[1]:.4f}")

"""## Performance Comparisom"""

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
import pandas as pd

# Assuming y_train_resampled contains true labels
true_labels = y_train_resampled

# Calculate Silhouette Score for each clustering algorithm
silhouette_kmeans = silhouette_score(X_train_encoded, kmeans_labels)
silhouette_agg = silhouette_score(X_train_encoded, agg_labels)
silhouette_dbscan = silhouette_score(X_train_encoded, dbscan_labels)

# Calculate Calinski-Harabasz Index for each clustering algorithm
calinski_harabasz_kmeans = calinski_harabasz_score(X_train_encoded, kmeans_labels)
calinski_harabasz_agg = calinski_harabasz_score(X_train_encoded, agg_labels)
calinski_harabasz_dbscan = calinski_harabasz_score(X_train_encoded, dbscan_labels)

# Calculate Davies-Bouldin Index for each clustering algorithm
davies_bouldin_kmeans = davies_bouldin_score(X_train_encoded, kmeans_labels)
davies_bouldin_agg = davies_bouldin_score(X_train_encoded, agg_labels)
davies_bouldin_dbscan = davies_bouldin_score(X_train_encoded, dbscan_labels)

# Calculate Adjusted Rand Index for each clustering algorithm
adjusted_rand_kmeans = adjusted_rand_score(true_labels, kmeans_labels)
adjusted_rand_agg = adjusted_rand_score(true_labels, agg_labels)
adjusted_rand_dbscan = adjusted_rand_score(true_labels, dbscan_labels)

# Create a comparison table
comparison_table = pd.DataFrame({
    'Metric': ['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Adjusted Rand Index'],
    'K-Means': [silhouette_kmeans, calinski_harabasz_kmeans, davies_bouldin_kmeans, adjusted_rand_kmeans],
    'Agglomerative': [silhouette_agg, calinski_harabasz_agg, davies_bouldin_agg, adjusted_rand_agg],
    'DBSCAN': [silhouette_dbscan, calinski_harabasz_dbscan, davies_bouldin_dbscan, adjusted_rand_dbscan]
})

# Print the comparison table
print("Comparison Table:")
print(comparison_table)

"""## K-means Visualization"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_and_evaluate_clusters(algorithm, data, n_clusters):
    # Dimensionality reduction
    if algorithm == 'PCA':
        reducer = PCA(n_components=2)
    elif algorithm == 't-SNE':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        raise ValueError("Unsupported algorithm")

    X_reduced = reducer.fit_transform(data)

    # KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans_labels = kmeans.fit_predict(X_reduced)

    # Silhouette Score
    silhouette_score_value = silhouette_score(X_reduced, kmeans_labels)
    print(f"{algorithm} Silhouette Score: {silhouette_score_value:.4f}")

    # Visualization
    plt.figure(figsize=(10, 5))
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.8)
    plt.title(f'{algorithm} - Clusters (Silhouette Score: {silhouette_score_value:.4f})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.colorbar(label='Cluster Assignment')
    plt.show()

# Usage
visualize_and_evaluate_clusters('PCA', X_train_encoded, n_clusters=2)
visualize_and_evaluate_clusters('t-SNE', X_train_encoded, n_clusters=2)

"""## Agglomerative Clustering Visualization"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_and_evaluate_agglomerative_clusters(algorithm, data, n_clusters):
    # Dimensionality reduction
    if algorithm == 'PCA':
        reducer = PCA(n_components=2)
    elif algorithm == 't-SNE':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        raise ValueError("Unsupported algorithm")

    X_reduced = reducer.fit_transform(data)

    # Agglomerative clustering
    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)
    ag_labels = agglomerative.fit_predict(X_reduced)

    # Silhouette Score
    silhouette_score_value = silhouette_score(X_reduced, ag_labels)
    print(f"{algorithm} Silhouette Score: {silhouette_score_value:.4f}")

    # Visualization
    plt.figure(figsize=(10, 5))
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=ag_labels, cmap='viridis', alpha=0.8)
    plt.title(f'{algorithm} - Clusters (Silhouette Score: {silhouette_score_value:.4f})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.colorbar(label='Cluster Assignment')
    plt.show()

# Usage
visualize_and_evaluate_agglomerative_clusters('PCA', X_train_encoded, n_clusters=2)
visualize_and_evaluate_agglomerative_clusters('t-SNE', X_train_encoded, n_clusters=2)

"""## DB-Scan Visualization"""

from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_and_evaluate_dbscan_clusters(algorithm, data, eps, min_samples):
    # Dimensionality reduction
    if algorithm == 'PCA':
        reducer = PCA(n_components=2)
    elif algorithm == 't-SNE':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        raise ValueError("Unsupported algorithm")

    X_reduced = reducer.fit_transform(data)

    # DBSCAN clustering
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    dbscan_labels = dbscan.fit_predict(X_reduced)

    # Silhouette Score
    silhouette_score_value = silhouette_score(X_reduced, dbscan_labels)
    print(f"{algorithm} Silhouette Score: {silhouette_score_value:.4f}")

    # Visualization
    plt.figure(figsize=(10, 5))
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.8)
    plt.title(f'{algorithm} - Clusters (Silhouette Score: {silhouette_score_value:.4f})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.colorbar(label='Cluster Assignment')
    plt.show()

# Usage
visualize_and_evaluate_dbscan_clusters('PCA', X_train_encoded, eps=0.5, min_samples=5)
visualize_and_evaluate_dbscan_clusters('t-SNE', X_train_encoded, eps=0.5, min_samples=5)